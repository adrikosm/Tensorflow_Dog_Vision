# -*- coding: utf-8 -*-
"""Dog_vision_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kv_vhVWeYuZsaTRn8GWA7Ux2HJ4rhC0r

# **Dog Breed Identification**

This notebook build an end-to-end image classifier using Tensorflow 2.0 and TensorFlow Hub

### 1. Problem
Identifying the breed of a dog given an image of a dog

### 2. Data
The data we are using is from kaggles dog breed identification competition.

https://www.kaggle.com/c/dog-breed-identification/data

### 3. Evaluation
The evaluation is a file with prediction probabilities for each dog breed of each test image


### 4. Features
Some basic information about the data:
* We are dealing with images (unstructured data) so it is probably best we use deep learning/transfer learning.
* There are 120 breeds of dogs (this means there are 120 different classes)
* There are around 10000+ images in the 
training set(this images have labels)
* There are around 10000+ images in the test set(these images have no labels,because we will want to predict them)
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# %matplotlib inline

# Turn categorical variables into numbers and fill missing
for label , content in df_tmp.items():
  if not pd.api.types.is_numeric_dtype(content):
    # Add a binary column to indicate whetcher sample had missing value
    df_tmp[label+"_is_missing_"] = pd.isnull(content)
    # Turn categories into numbers and add + 1
    df_tmp[label] = df_tmp[label].str.replace(',','','.')
    df_tmp[label] = pd.Categorical(content).codes + 1

# Set default figsize to 10,6
plt.rcParams["figure.figsize"] = (10,6)

"""## Get the workspace ready

* Import Tensorflow 
* Import TensorFlow Hub
* Make sure we are using gpu
"""

# Import tensorflow
import tensorflow as tf
print(f'TensorFlow version: {tf.__version__}')

# Import tensorflow hub
import tensorflow_hub as hub
print(f'TensorFlow hub version: {hub.__version__}')

# Check GPU availability
print("GPU","Availabe (yes)" if tf.config.list_physical_devices("GPU") else "not available")

"""# Getting our data ready (turing it into Tensors)


---
As with all machine learning models , our data has to be in numeric format.
So we will turn our images into tensors (numerical representation)
- Lets start by accessing our data and checking out the labels
"""

# Checkout the labels of our data

labels_csv = pd.read_csv("/content/drive/MyDrive/machine_learning/Dog_vision_project/labels.csv")

print(labels_csv.describe())

labels_csv.head()

"""- ID of the labels = the name of the images"""

# How many images are there of each breed
labels_csv["breed"].value_counts().plot.bar(figsize=(20,10))

labels_csv["breed"].value_counts().median()

# lets view an image
from IPython.display import Image

Image("/content/drive/MyDrive/machine_learning/Dog_vision_project/train/001513dfcb2ffafc82cccf4d8bbaba97.jpg")

"""### Getting images and their labels

Lets get a list of all of our image file pathnames

"""

# Create pathnames of image id

filename = ["drive/MyDrive/machine_learning/Dog_vision_project/train/" + fname + ".jpg" for fname in labels_csv.id]

filename[:10]

# Check whether number of file names matches number of actual image files
import os
if len(os.listdir("/content/drive/MyDrive/machine_learning/Dog_vision_project/train")) == len(filename):
  print("Filename matches actual amount of train files !you can proceed")
else:
  print("Filename does not matches actual ammount of train files !do not proceed")

# One more check
Image(filename[9000])

labels_csv["breed"][9000]

"""# Turning Data labels Into Numbers

---

Since we now have our training image filepaths in a list lets prepare our labels
"""

labels = labels_csv["breed"].to_numpy() # Transform data into numpy array
labels

len(labels)

# See if  number of labels matches the numbers of filenames
if len(labels) == len(filename):
  print("Number of labels matches number of filenames, you may proceed")
else:
  print("Number of labels does not matches number of filenames,do not proceed")

# Find unique label values
unique_breeds = np.unique(labels)
len(unique_breeds)

# Turn a single label into an array of booleans
print(labels[0])
labels[0] == unique_breeds

# Turn every label into a boolean array
boolean_labels = [label == unique_breeds for label in labels]
boolean_labels[:2]

len(boolean_labels)

# Example : Turning boolean array into integers
print(labels[0]) # original label
print(np.where(unique_breeds==labels[0])) # Index where label occurs
print(boolean_labels[0].argmax()) # Index where label occurs in boolean array
print(boolean_labels[0].astype(int)) # There will be a 1 where the sample label occurs

print(labels[2])
print(boolean_labels[2].astype(int))

filename[:5]

"""# Creating our own validation set
Since the dataset from kaggle does not come with a validation test , we are going to create our own
"""

# Setup X and u variables
X = filename
y = boolean_labels

"""We are going to start off experimenting with ~1000 images and increase as needed."""

NUM_IMAGES = 1000 #@param {type:"slider", min:1000, max:10000, step:1000}

# Lets split our data into train and validation
from sklearn.model_selection import train_test_split

# Split thme into training and validation of total size NUM_IMAGES
X_train,X_val,y_train,y_val = train_test_split(X[:NUM_IMAGES],
                                               y[:NUM_IMAGES],
                                               test_size=0.2,
                                               random_state=42)
len(X_train),len(X_val),len(y_train),len(y_val)

# Lets have a look at the training data
X_train[:5]

y_train[:2]

"""# Preprocessing Images (Turning images into Tensors)

---
To preprocess our images into Tensors we are going to write a function which does a few things:
1. Take an image filepath as input
2. Use TensorFlow to read the file and save it to a variable,`image`
3. Turn our `image` (jpg) into Tensors
4. Resize the `image` to be a shape of (224,224)
5. Return the modifier `image`

**Before we do lets see what importing an image looks like**

"""

# Convert Image to Numpy array
from matplotlib.pyplot import imread
image = imread(filename[42])

image.shape

image[:1]

# Turn image into a tensor
tf.constant(image)[:1]

"""Now we have seen what an image looks like as a tensor now lets make a function to preprocess them"""

# Define image size
IMG_SIZE = 224

# Create a function for preprocessing images

def process_image(image_path,img_size=IMG_SIZE):
  """
  Takes an image file path and turns the image
  into a Tensor
  sets max image size to 224, 224
  """
  image = tf.io.read_file(image_path)
  # Turn the jpg image into a numerical Tensor with 3 color channels (RGB)
  image = tf.image.decode_jpeg(image,channels=3)
  # Convert the color channel values from 0-255 to 0-1 values
  # Normalizing the image (converting from 0-255 to 0-1) makes the computing much faster
  image = tf.image.convert_image_dtype(image,tf.float32)
  # Resize the image to our desired value (224,224)
  image = tf.image.resize(image,size=[img_size,img_size])

  return image

"""## Turning our data into batches

Why turn our data into batches?



---
- lets say you are trying to process 10.000+ images in one go they all might not fit into memory.
- So thats why we do about 32 (batch size)images at a time (you cant adjust the batch size if need be)

In order to use TensorFlow effectively, we need our data in the form of Tensor tuples which look like this:
* `(image,label)` pairs 

"""

# Create a simple function to return a tuple of tensors (image, label)
def get_image_label(image_path,label):
  """
  Takes an image path name and the associated label,
  process the image and returns a type of (image,label) 
  """
  image = process_image(image_path)
  return image,label

# Demo of the above
(process_image(X[42]),tf.constant(y))

"""Now we have got a way to turn the data into tensor tuples, in the form `(image,label)` lets make a function to turn all of our data (`x` and `y`) into batches of 32"""

# Define the batch size, 32 is a good start
BATCH_SIZE = 32

# Create a function to turn data into batches
def create_data_batches(X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):
  """
  Creates batches of data out of image (X) and label (y) pairs.
  Shuffles the data if it's training data but doesn't shuffle if it's validation data.
  Also accepts test data as input (no labels).
  """
  # If the data is a test dataset, we probably don't have have labels
  if test_data:
    print("Creating test data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X))) # only filepaths (no labels)
    data_batch = data.map(process_image).batch(BATCH_SIZE)
    return data_batch
  
  # If the data is a valid dataset, we don't need to shuffle it
  elif valid_data:
    print("Creating validation data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), # filepaths
                                               tf.constant(y))) # labels
    data_batch = data.map(get_image_label).batch(BATCH_SIZE)
    return data_batch

  else:
    print("Creating training data batches...")
    # Turn filepaths and labels into Tensors
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),
                                               tf.constant(y)))
    # Shuffling pathnames and labels before mapping image processor function is faster than shuffling images
    data = data.shuffle(buffer_size=len(X))

    # Create (image, label) tuples (this also turns the iamge path into a preprocessed image)
    data = data.map(get_image_label)

    # Turn the training data into batches
    data_batch = data.batch(BATCH_SIZE)
  return data_batch

# Create training and validation data batches
train_data = create_data_batches(X_train,y_train)
val_data = create_data_batches(X_val,y_val,valid_data=True)

train_data.element_spec

val_data.element_spec

"""## Visualizing Data batches

Our data is now in batches, however these can be a little hard to understand
so lets visualize them
"""

import matplotlib.pyplot as plt

# Create a function for viewing images in a data batch
def show_25_images(images, labels):
  """
  Displays a plot of 25 images and their labels from a data batch.
  """
  # Setup the figure
  plt.figure(figsize=(12, 10))
  # Loop through 25 (for displaying 25 images)
  for i in range(25):
    # Create subplots (5 rows, 5 columns)
    ax = plt.subplot(5, 5, i+1)
    # Display an image 
    plt.imshow(images[i])
    # Add the image label as the title
    plt.title(unique_breeds[labels[i].argmax()])
    # Turn the grid lines off
    plt.axis("off")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# train_images,train_labels = next(train_data.as_numpy_iterator())
# train_images,train_labels

len(train_images),len(train_labels)

# Now lets visualize the data in a training batch
show_25_images(train_images,train_labels)

# Now lets visualize our validation set
val_images , val_labels = next(val_data.as_numpy_iterator())
show_25_images(val_images,val_labels)

"""# **Building a model**
Before we build a model , there are a few things we need to define :
* The input shape (our images shape in the form of Tensors) to our model.
* The output shape (image labels in the form of Tensors) of our model.
* The URL of the model we want to use
"""

# Set up input shape to the model
INPUT_SHAPE =[None,IMG_SIZE,IMG_SIZE,3] # Batch , height , width , color channels

# Setup output shape of our model
OUTPUT_SHAPE = len(unique_breeds)

# Setup model URL from tensorflow hub
MODEL_URL = "https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4"

"""Now we have got our inputs outputs and model ready to go.Lets put them together into a Keras Deep learning model.
Knowing this lets create a function which:
* Takes the input , output shape and the model we have chosen as parameters
* Defines the layers in a keras model in sequential fashion (do this first then that)
* Complies the model ( says how it should be evaluated and improved)
* Build the model (tells the model the input shape it will be gettin)
* Finnaly returns the model

All of these steps can be found on keras tensorflow documentation
"""

# Create a function which builds a Keras model
def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
  print("Building model with:", MODEL_URL)

  # Setup the model layers
  model = tf.keras.Sequential([
    hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)
    tf.keras.layers.Dense(units=OUTPUT_SHAPE,
                          activation="softmax") # Layer 2 (output layer)
  ])

  # Compile the model
  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=["accuracy"]
  )

  # Build the model
  model.build(INPUT_SHAPE)

  return model

model = create_model()
model.summary()

"""## Creating callbacks
Call nacls are helper functioons a model can use during trainign to do such things as save its progress check its progress or stop training early if a model stops imroving

---
We will create two callbacks one for Tensorboard which helps track our models progrss and another for early stopping which prevents our model from training for too long

### TensorBoard Callback
To setup a TensorBoard callback we need to do 3 things
1. Load the Tensorboard notebook extension
2. Create a tensorboard callback which is able to save logs to a directory
and pass it to our models `fit()` function
3. Visualize our models training logs with the %`tensorboard` magic function (we will do this after model training)
"""

# Commented out IPython magic to ensure Python compatibility.
# Load TensorBoard notebook extension
# %load_ext tensorboard

import datetime

# Create a function to build a TensorBoard callback
def create_tensorboard_callback():
  # Create a log directory for storing TensorBoard logs
  logdir = os.path.join("/content/drive/MyDrive/machine_learning/Dog_vision_project/logs",
                        # Make it so the logs get tracked whenever we run an experiment
                        datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
  return tf.keras.callbacks.TensorBoard(logdir)

"""### Early Stopping Callback
Early Stopping helps our model from overfitting by stopping training if a certnain evaluation metric stops imporving
"""

# Create early stopping callback
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",
                                                  patience=3)

"""## Training a model (on subset of data)

Our first model is only going to train on 1000 images , to make sure everything is working.
"""

NUM_EPOCHS = 100 #@param {type:"slider",min:10,max:100,step:10}

# Ckech to make sure we are still running on a gpu

print("GPU","Availabe (yes) " if tf.config.list_logical_devices("GPU") else "not available")

"""Lets create a function which trains a model.
* Create a model using `create_model()`
* Setup a TenosrBoard callback using `create_tensorboard_callback()`
* Call the `fit()` function on our model passing it the training data,
validation data , number of epochs to train for (`NUM_EPOCHS`) and the
callbacks we would like to use
* Return the model
"""

# Build a function to train and return a trained model
def train_model():
  """
  Trains a given model and returns the trained version.
  """
  # Create a model
  model = create_model()

  # Create new TensorBoard session everytime we train a model
  tensorboard = create_tensorboard_callback()

  # Fit the model to the data passing it the callbacks we created
  model.fit(x=train_data,
            epochs=NUM_EPOCHS,
            validation_data=val_data,
            validation_freq=1,
            callbacks=[tensorboard, early_stopping])
  # Return the fitted model
  return model

# %%time
# # Fit the model to the data
# model = train_model()

"""**It look like our model is overfitting since it is performing far
better in the training dataset than the validation dataset,what are some
ways to prevent model overfitting in deep learning neural networks**


---

**NOTE** overfitting to begin with is a good thing it means our model is learning

### Checking the TensorBoard logs
The tensorboard magic function `(%tensorboard)` will acess the logs directory we created earlier and visualize its contents.
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/drive/MyDrive/machine_learning/Dog_vision_project/logs

"""# Making and evaluating predictions using a trained model"""

# Make predictions on the validation data (not used to train on)
predictions = model.predict(val_data,verbose=1)
predictions

np.sum(predictions[0])

# First prediction
index = 10
#print(predictions[index])
print(f'Max value (probability of prediction): {np.max(predictions[index])}')
print(f'Sum: {np.sum(predictions[index])} ')
print(f'Max index: {np.argmax(predictions[index])}')
print(f'Predicted label: {unique_breeds[np.argmax(predictions[index])]}')

"""Having the above functionality is great but we want to be able to do it at scale.
Also it would be even better if we could see the image the prediction is being made on


---
**Note:** Prediction probabilities are alos known as confidence levels 

"""

# Turn prediction probabilities  into the respective label(easier to understand)
def get_pred_label(prediction_probabilities):
  """
  Turns an array of prediciton probabilities into
  a label.
  """
  return unique_breeds[np.argmax(prediction_probabilities)]

# Get a predicted label based on an array of prediction probabilities
pred_label = get_pred_label(predictions[50])
pred_label

"""Now since our validation data set is still in a batch we will have to unbatch it in order to make predictions on validation images and then compare those predictions to the valdiation labels (truth labels)"""

# Create a function to unbatch a batch dataset.
def unbatch_data(data):
  """
  Takes a batched data set of (images,labels) Tensors and
  returns separate arrays of images and labels
  """
  images = []
  labels = []

  for image , label in data.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(unique_breeds[np.argmax(label)])
  return images,labels

# unbatch validation data
val_images , val_labels = unbatch_data(val_data)
val_images[0],val_labels[0]

get_pred_label(val_labels[0])

"""Now we have ways to get:
* Prediction labels
* Validation labels (truth labels)
* Validation images

---



Lets make some functions to make these a bit more visual. 
We will create a function which:
- Takes an array of prediction probabilities ,an array of truth labels and an array of images and integers.
- Convert the prediction probabilities to a predicted label
- Plot the predicted label, its predicted probabiliti , the truth label and the target image on a single plot.
"""

def plot_pred(prediction_probabilities, labels, images, n=1):
  """
  View the prediction , ground truth and image for sample n
  """
  pred_prob,true_label,image = prediction_probabilities[n],labels[n],images[n]

  # Get the pred label
  pred_label = get_pred_label(pred_prob)

  # Plot image and remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])
  
  # Change the color of the title depending on if the prediction is right or wrong
  if pred_label == true_label:
    color="green"
  else:
      color="red"
  # Change plot title to be predicted, probability of prediction and truth label
  plt.title( "Predicted: {}  {:2.0f}%  Actual: {}".format(pred_label,
                                     np.max(pred_prob)*100,
                                     true_label),
                                     color = color)

plot_pred(prediction_probabilities=predictions,
          labels=val_labels,
          images=val_images,
          n=9)

"""Now we have got one function to visualize our models top prediction, lets make another to view our models top 10 predictions

---


This function will:
- Take an input of prediction_probabilities array and a ground truth array and an integer
- Find the predicted label using `get_pred_label()`
- Find the top 10:
  * Prediction probabilities indexes
  * Prediction probabilities values
  * Prediction labels
- Plot the top 10 prediction probability values and labels, coloring the true label green 
"""

def plot_pred_conf(prediction_probabilities, labels, n=1):
  """
  Plus the top 10 highest prediction confidences along with the truth label for sample n.
  """
  pred_prob, true_label = prediction_probabilities[n], labels[n]

  # Get the predicted label
  pred_label = get_pred_label(pred_prob)

  # Find the top 10 prediction confidence indexes
  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]
  # Find the top 10 prediction confidence values
  top_10_pred_values = pred_prob[top_10_pred_indexes]
  # Find the top 10 prediction labels
  top_10_pred_labels = unique_breeds[top_10_pred_indexes]

  # Setup plot
  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),
                     top_10_pred_values,
                     color="grey")
  plt.xticks(np.arange(len(top_10_pred_labels)),
             labels=top_10_pred_labels,
             rotation="vertical")
  
  # Change color of true label
  if np.isin(true_label, top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color("green")
  else:
    pass

plot_pred_conf(prediction_probabilities=predictions,
               labels=val_labels,
               n=9)

"""Now we have got some functions to help us visualize our predictions and evaluate the model , lets chech out a few"""

# Let's check out a few predictions and their different values
i_multiplier = 20
num_rows = 3
num_cols = 2
num_images = num_rows*num_cols
plt.figure(figsize=(10*num_cols, 5*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_pred(prediction_probabilities=predictions,
            labels=val_labels,
            images=val_images,
            n=i+i_multiplier)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_pred_conf(prediction_probabilities=predictions,
                 labels=val_labels,
                 n=i+i_multiplier)
plt.tight_layout(h_pad=1.0)
plt.show()

"""## Saving and reloding a trained model"""

# Create a function to save a model 
def save_model(model,suffix=None):
  """
  Saves a given model in the models directory and appends
  a suffix (string)
  """
  # Create a model directory pathname with current time
  modeldir = os.path.join("/content/drive/MyDrive/machine_learning/Dog_vision_project/models",
                          datetime.datetime.now().strftime("%Y%m%d-%H%M%s"))
  model_path = modeldir + "-" + suffix + ".h5" # H5 is a save weight (format)
  print(f'Saving model to: {model_path}')
  model.save(model_path)
  return model_path

# Create a function to load a trained model
def load_model(model_path):
  """
  Loads a saved model from a specified path.
  """
  print(f"Loading saved model from: {model_path}")
  model = tf.keras.models.load_model(model_path,
                                     custom_objects={"KerasLayer":hub.KerasLayer})
  return model

"""Now we have functions to save and load a trained model so lets make sure they work"""

# Save model trained on 1000 images
save_model(model,suffix="1000-images-mobilenetv2-Adam")

# Load a trained model
loaded_1000_image_model = load_model("/content/drive/MyDrive/machine_learning/Dog_vision_project/models/20210128-20061611864383-1000-images-mobilenetv2-Adam.h5")

# Evaluate the pre-saved model
model.evaluate(val_data)

# Evaluate the loaded model
loaded_1000_image_model.evaluate(val_data)

"""## Training a big dog model (on the full data)"""

len(X),len(y)

# Create a data batch with the full data set

full_data = create_data_batches(X,y)

full_data

# Create a model for full model
full_model = create_model()

# Create full model callbacks
full_model_tensorboard = create_tensorboard_callback()
# No validation set when training on all the data , so
# we cant monitor validation accuracy
full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor="accuracy",
                                                             patience=3)

"""**Important Note**: Running the cell below will take a little while (maybe up to 30 minutes) because the GPU we are using in the runtime has to laod all of the images into memory."""

# %%time
# # Fit the full model to the full data
# full_model.fit(x=full_data,
#                epochs=NUM_EPOCHS,
#                callbacks=[full_model_tensorboard,full_model_early_stopping])

# save_model(full_model,suffix="full-image-set-mobilenetV2-Adam")

# Load the full model
loaded_full_model = load_model("/content/drive/MyDrive/machine_learning/Dog_vision_project/models/20210128-21341611869694-full-image-set-mobilenetV2-Adam.h5")

"""# Making predictions on the test data set

---

Since our model has been trained on images in the form of TensorBatches, to make predictions on the test data we will have to get it in the same format

-Luckily we created `create_data_batches()`earlier which can take a list of filenames as input and covnvert them into TensorBatches
- To make predictions on the test data we will:
  * Get the test image filenames
  * Convert the filenames into test data batches using `create_data_batches()` and setting the `test_data` parameter to `True` (since the test data does not have labels)
  * Make a predictions array by passing the test batches to the `predict()` method called on our model.
"""

# Load test image filenames
test_path = "/content/drive/MyDrive/machine_learning/Dog_vision_project/test/"
test_filenames = [test_path + fname for fname in os.listdir(test_path)]
test_filenames[:5]

len(test_filenames)

# Create test data batch
test_data = create_data_batches(test_filenames,test_data=True)

test_data

"""**Note** calling `predict()` on our full model and passing it the test data batch will take a long time to run (about an hour)"""

# %%time
# # Make predictions on test data batch using the loaded full model
# test_predictions = loaded_full_model.predict(test_data,
#                                              verbose=1)

test_predictions[:1]

# # Save test predictions to a csv file
# np.savetxt("/content/drive/MyDrive/machine_learning/Dog_vision_project/prediction_array.csv", test_predictions,delimiter=",")

# Load predictions from csv file
test_predictions = np.loadtxt("/content/drive/MyDrive/machine_learning/Dog_vision_project/prediction_array.csv",delimiter=",")

test_predictions[:1]

"""## Preparing test data set predictions fro Kaggle
Looking at the Kaggles sample submission we find that it wants
our models prediction probability outputs in a DataFrame with
an ID column and a column with each dog breed


---
To get this data in this format we will:
* Create a pandas DataFrame with and id column as well as a column for each dog breed
* We will add data to the id column by extracting the test image id from their file paths
* Add data (the prediction probabilities) to each dog breed columns
* Export data to a csv to submit to Kaggle

"""

# Creata a pandas DataFrame with empty columns
preds_df = pd.DataFrame(columns=["id"]+list(unique_breeds))
preds_df

# Append test image ID to prediction dataframe
test_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]
#test_ids

preds_df["id"] = test_ids
preds_df

# Add the prediction probabilities to each dog breed column
preds_df[list(unique_breeds)] = test_predictions
preds_df.head()

# Save our predictions dataframe to csv for submission to Kaggle
preds_df.to_csv("/content/drive/MyDrive/machine_learning/Dog_vision_project/full_model_predictions_submission_1.csv",
                index=False)

"""## Making predictions on custom images

To make predictions on custom images we will:
* Get the filepaths of our own images.
* Turn the filepaths into databatches using `create_data_batches()`. And since our custom images wont have labels we set the `test_data` parameter to `True`.
* Pass the custom image data batch to our models `predict()` method
* Convert the prediction output probabilities to prediction labels.
* Compare the predicted labels to the custom images

"""

# Get the custom image filepaths
custom_path = "/content/drive/MyDrive/machine_learning/Dog_vision_project/extra-dog-photos/"
custom_image_paths = [custom_path + fname for fname in os.listdir(custom_path)]

custom_image_paths

# Turn custom images into batch datasets 
custom_data = create_data_batches(custom_image_paths, test_data=True)
custom_data

# Make predictions on the custom data
custom_preds = loaded_full_model.predict(custom_data)
#custom_preds

custom_preds.shape

# Get custom image prediction labels
custom_pred_labels = [get_pred_label(custom_preds[i]) for i in range(len(custom_preds))]
custom_pred_labels

# Get custom images (unbatch function wont work since there are no labels)
custom_images = []

# Loop through unbatched data
for image in custom_data.unbatch().as_numpy_iterator():
  custom_images.append(image)

# Check custom image predictions
plt.figure(figsize=(20, 10))
for i, image in enumerate(custom_images):
  plt.subplot(1, len(custom_images), i+1)
  plt.xticks([])
  plt.yticks([])
  plt.title(custom_pred_labels[i])
  plt.imshow(image)